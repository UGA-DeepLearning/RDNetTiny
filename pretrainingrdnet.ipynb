{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-22T00:49:21.311042Z",
     "iopub.status.busy": "2025-04-22T00:49:21.310495Z",
     "iopub.status.idle": "2025-04-22T00:49:21.316355Z",
     "shell.execute_reply": "2025-04-22T00:49:21.315682Z",
     "shell.execute_reply.started": "2025-04-22T00:49:21.311019Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "from functools import partial\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingWarmRestarts\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from matplotlib import cm\n",
    "\n",
    "import timm\n",
    "from timm.layers import EffectiveSEModule, DropPath, LayerNorm2d\n",
    "from timm.models import named_apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T00:49:09.888512Z",
     "iopub.status.busy": "2025-04-22T00:49:09.887942Z",
     "iopub.status.idle": "2025-04-22T00:49:09.946661Z",
     "shell.execute_reply": "2025-04-22T00:49:09.945915Z",
     "shell.execute_reply.started": "2025-04-22T00:49:09.888489Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.mps.is_available():\n",
    "    device = 'mps'\n",
    "elif torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T00:49:09.947762Z",
     "iopub.status.busy": "2025-04-22T00:49:09.947534Z",
     "iopub.status.idle": "2025-04-22T00:49:10.002366Z",
     "shell.execute_reply": "2025-04-22T00:49:10.001253Z",
     "shell.execute_reply.started": "2025-04-22T00:49:09.947745Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seed = 29\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T00:49:10.003637Z",
     "iopub.status.busy": "2025-04-22T00:49:10.003322Z",
     "iopub.status.idle": "2025-04-22T00:49:10.007916Z",
     "shell.execute_reply": "2025-04-22T00:49:10.007152Z",
     "shell.execute_reply.started": "2025-04-22T00:49:10.003604Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = './data.cifar10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Specific Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T00:50:33.138540Z",
     "iopub.status.busy": "2025-04-22T00:50:33.138282Z",
     "iopub.status.idle": "2025-04-22T00:50:33.800939Z",
     "shell.execute_reply": "2025-04-22T00:50:33.800361Z",
     "shell.execute_reply.started": "2025-04-22T00:50:33.138523Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "timm_model = timm.create_model('rdnet_tiny.nv_in1k', pretrained=True)\n",
    "data_config = timm.data.resolve_model_data_config(timm_model)\n",
    "transform = timm.data.create_transform(**data_config, is_training=True)\n",
    "test_transform = timm.data.create_transform(**data_config, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T00:50:36.236848Z",
     "iopub.status.busy": "2025-04-22T00:50:36.236574Z",
     "iopub.status.idle": "2025-04-22T00:50:36.243105Z",
     "shell.execute_reply": "2025-04-22T00:50:36.242358Z",
     "shell.execute_reply.started": "2025-04-22T00:50:36.236827Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Confiuration :{'input_size': (3, 224, 224), 'interpolation': 'bicubic', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225), 'crop_pct': 0.9, 'crop_mode': 'center'},\n",
      "Data Transformation: Compose(\n",
      "    RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    ColorJitter(brightness=(0.6, 1.4), contrast=(0.6, 1.4), saturation=(0.6, 1.4), hue=None)\n",
      "    MaybeToTensor()\n",
      "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
      "),\n",
      "Test Transformation: Compose(\n",
      "    Resize(size=248, interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    MaybeToTensor()\n",
      "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data Confiuration :{data_config},\\nData Transformation: {transform},\\nTest Transformation: {test_transform}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the pretrained weights. Hugging face models use .bin format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T01:01:17.912354Z",
     "iopub.status.busy": "2025-04-22T01:01:17.911672Z",
     "iopub.status.idle": "2025-04-22T01:01:18.030029Z",
     "shell.execute_reply": "2025-04-22T01:01:18.029462Z",
     "shell.execute_reply.started": "2025-04-22T01:01:17.912331Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# torch.save(timm_model.state_dict(), 'rdnet_tiny_pretrained.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducing the same transformation locally not relying on timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T00:51:24.402399Z",
     "iopub.status.busy": "2025-04-22T00:51:24.402091Z",
     "iopub.status.idle": "2025-04-22T00:51:24.408147Z",
     "shell.execute_reply": "2025-04-22T00:51:24.407442Z",
     "shell.execute_reply.started": "2025-04-22T00:51:24.402378Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(\n",
    "        size=(224, 224),\n",
    "        scale=(0.08, 1.0),\n",
    "        ratio=(0.75, 1.3333),\n",
    "        interpolation=InterpolationMode.BICUBIC\n",
    "    ),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=(0.6, 1.4),\n",
    "        contrast=(0.6, 1.4),\n",
    "        saturation=(0.6, 1.4)\n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225)\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(\n",
    "        size=248,\n",
    "        interpolation=InterpolationMode.BICUBIC,\n",
    "        antialias=True\n",
    "    ),\n",
    "    transforms.CenterCrop(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225)\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In case there is a ssl certificate issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR 10 DATA DOWNLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T00:51:31.016137Z",
     "iopub.status.busy": "2025-04-22T00:51:31.015839Z",
     "iopub.status.idle": "2025-04-22T00:51:41.222486Z",
     "shell.execute_reply": "2025-04-22T00:51:41.221934Z",
     "shell.execute_reply.started": "2025-04-22T00:51:31.016115Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainset = CIFAR10(root=DATA_ROOT, train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = CIFAR10(root=DATA_ROOT, train=False, download=True, transform=test_transform)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T01:09:16.927733Z",
     "iopub.status.busy": "2025-04-22T01:09:16.927446Z",
     "iopub.status.idle": "2025-04-22T01:09:18.551701Z",
     "shell.execute_reply": "2025-04-22T01:09:18.550921Z",
     "shell.execute_reply.started": "2025-04-22T01:09:16.927711Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "full_dataset = CIFAR10(root=DATA_ROOT, train=True, download=True)\n",
    "targets = np.array(full_dataset.targets)\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=5000, random_state=29)\n",
    "train_idx, val_idx = next(sss.split(np.zeros(len(targets)), targets))\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "val_dataset   = Subset(full_dataset, val_idx)\n",
    "\n",
    "train_dataset.dataset.transform = train_transform\n",
    "val_dataset.dataset.transform   = test_transform\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "test_dataset = CIFAR10(root=DATA_ROOT, train=False, download=True, transform=test_transform)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T00:51:43.439600Z",
     "iopub.status.busy": "2025-04-22T00:51:43.438878Z",
     "iopub.status.idle": "2025-04-22T00:51:43.463362Z",
     "shell.execute_reply": "2025-04-22T00:51:43.462784Z",
     "shell.execute_reply.started": "2025-04-22T00:51:43.439577Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "__all__ = [\"RDNet\"]\n",
    "\n",
    "\n",
    "class RDNetClassifierHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        num_classes: int,\n",
    "        drop_rate: float = 0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.num_features = in_features\n",
    "\n",
    "        self.norm = nn.LayerNorm(in_features)\n",
    "        self.drop = nn.Dropout(drop_rate)\n",
    "        self.fc = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def reset(self, num_classes):\n",
    "        self.fc = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x, pre_logits: bool = False):\n",
    "        x = x.mean([-2, -1])\n",
    "        x = self.norm(x)\n",
    "        x = self.drop(x)\n",
    "        if pre_logits:\n",
    "            return x\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchifyStem(nn.Module):\n",
    "    def __init__(self, num_input_channels, num_init_features, patch_size=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, num_init_features, kernel_size=patch_size, stride=patch_size),\n",
    "            LayerNorm2d(num_init_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.stem(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module): #this is Feature MIXER\n",
    "    \"\"\"D == Dw conv, N == Norm, F == Feed Forward, A == Activation\"\"\"\n",
    "    def __init__(self, in_chs, inter_chs, out_chs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_chs, in_chs, groups=in_chs, kernel_size=7, stride=1, padding=3),\n",
    "            LayerNorm2d(in_chs, eps=1e-6),\n",
    "            nn.Conv2d(in_chs, inter_chs, kernel_size=1, stride=1, padding=0),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(inter_chs, out_chs, kernel_size=1, stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class BlockESE(nn.Module): # this is Feature MIXER\n",
    "    \"\"\"D == Dw conv, N == Norm, F == Feed Forward, A == Activation\"\"\"\n",
    "    def __init__(self, in_chs, inter_chs, out_chs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_chs, in_chs, groups=in_chs, kernel_size=7, stride=1, padding=3),\n",
    "            LayerNorm2d(in_chs, eps=1e-6),\n",
    "            nn.Conv2d(in_chs, inter_chs, kernel_size=1, stride=1, padding=0),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(inter_chs, out_chs, kernel_size=1, stride=1, padding=0),\n",
    "            EffectiveSEModule(out_chs),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_input_features,\n",
    "        growth_rate,\n",
    "        bottleneck_width_ratio, # I think this is wide part of bottleneck\n",
    "        drop_path_rate,\n",
    "        drop_rate=0.0,\n",
    "        rand_gather_step_prob=0.0,\n",
    "        block_idx=0,\n",
    "        block_type=\"Block\",\n",
    "        ls_init_value=1e-6,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.drop_rate = drop_rate\n",
    "        self.drop_path_rate = drop_path_rate\n",
    "        self.rand_gather_step_prob = rand_gather_step_prob\n",
    "        self.block_idx = block_idx\n",
    "        self.growth_rate = growth_rate\n",
    "\n",
    "        self.gamma = nn.Parameter(ls_init_value * torch.ones(growth_rate)) if ls_init_value > 0 else None\n",
    "        growth_rate = int(growth_rate)\n",
    "        inter_chs = int(num_input_features * bottleneck_width_ratio / 8) * 8\n",
    "\n",
    "        if self.drop_path_rate > 0:\n",
    "            self.drop_path = DropPath(drop_path_rate)\n",
    "\n",
    "        self.layers = eval(block_type)(\n",
    "            in_chs=num_input_features,\n",
    "            inter_chs=inter_chs,\n",
    "            out_chs=growth_rate, # the concatentation of features\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, List):\n",
    "            x = torch.cat(x, 1)\n",
    "        x = self.layers(x)\n",
    "\n",
    "        if self.gamma is not None:\n",
    "            x = x.mul(self.gamma.reshape(1, -1, 1, 1))\n",
    "\n",
    "        if self.drop_path_rate > 0 and self.training:\n",
    "            x = self.drop_path(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DenseStage(nn.Sequential):\n",
    "    def __init__(self, num_block, num_input_features, drop_path_rates, growth_rate, **kwargs):\n",
    "        super().__init__()\n",
    "        for i in range(num_block):\n",
    "            layer = DenseBlock(\n",
    "                num_input_features=num_input_features,\n",
    "                growth_rate=growth_rate,\n",
    "                drop_path_rate=drop_path_rates[i],\n",
    "                block_idx=i,\n",
    "                **kwargs,\n",
    "            )\n",
    "            num_input_features += growth_rate\n",
    "            self.add_module(f\"dense_block{i}\", layer)\n",
    "        self.num_out_features = num_input_features\n",
    "\n",
    "    def forward(self, init_feature):\n",
    "        features = [init_feature]\n",
    "        for module in self:\n",
    "            new_feature = module(features)\n",
    "            features.append(new_feature)\n",
    "        return torch.cat(features, 1)\n",
    "\n",
    "\n",
    "class RDNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_init_features=64,\n",
    "        growth_rates=(64, 104, 128, 128, 128, 128, 224),\n",
    "        num_blocks_list=(3, 3, 3, 3, 3, 3, 3),\n",
    "        bottleneck_width_ratio=4,\n",
    "        zero_head=False,\n",
    "        in_chans=3,  # timm option [--in-chans]\n",
    "        num_classes=1000,  # timm option [--num-classes]\n",
    "        drop_rate=0.0,  # timm option [--drop: dropout ratio]\n",
    "        drop_path_rate=0.0,  # timm option [--drop-path: drop-path ratio]\n",
    "        checkpoint_path=None,  # timm option [--initial-checkpoint]\n",
    "        transition_compression_ratio=0.5,\n",
    "        ls_init_value=1e-6,\n",
    "        is_downsample_block=(None, True, True, False, False, False, True),\n",
    "        block_type=\"Block\",\n",
    "        head_init_scale: float = 1.,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert len(growth_rates) == len(num_blocks_list) == len(is_downsample_block)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        if isinstance(block_type, str):\n",
    "            block_type = [block_type] * len(growth_rates)\n",
    "\n",
    "        # stem\n",
    "        self.stem = PatchifyStem(in_chans, num_init_features, patch_size=4)\n",
    "\n",
    "        # features\n",
    "        self.feature_info = []\n",
    "        self.num_stages = len(growth_rates)\n",
    "        curr_stride = 4  # stem_stride\n",
    "        num_features = num_init_features\n",
    "        dp_rates = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(num_blocks_list)).split(num_blocks_list)]\n",
    "\n",
    "        dense_stages = []\n",
    "        for i in range(self.num_stages):\n",
    "            dense_stage_layers = []\n",
    "            if i != 0:\n",
    "                compressed_num_features = int(num_features * transition_compression_ratio / 8) * 8\n",
    "                k_size = stride = 1\n",
    "                if is_downsample_block[i]:\n",
    "                    curr_stride *= 2\n",
    "                    k_size = stride = 2\n",
    "                dense_stage_layers.append(LayerNorm2d(num_features))\n",
    "                dense_stage_layers.append(\n",
    "                    nn.Conv2d(num_features, compressed_num_features, kernel_size=k_size, stride=stride, padding=0)\n",
    "                )\n",
    "                num_features = compressed_num_features\n",
    "\n",
    "            stage = DenseStage(\n",
    "                num_block=num_blocks_list[i],\n",
    "                num_input_features=num_features,\n",
    "                growth_rate=growth_rates[i],\n",
    "                bottleneck_width_ratio=bottleneck_width_ratio,\n",
    "                drop_rate=drop_rate,\n",
    "                drop_path_rates=dp_rates[i],\n",
    "                ls_init_value=ls_init_value,\n",
    "                block_type=block_type[i],\n",
    "            )\n",
    "            dense_stage_layers.append(stage)\n",
    "            num_features += num_blocks_list[i] * growth_rates[i]\n",
    "\n",
    "            if i + 1 == self.num_stages or (i + 1 != self.num_stages and is_downsample_block[i + 1]):\n",
    "                self.feature_info += [\n",
    "                    dict(\n",
    "                        num_chs=num_features,\n",
    "                        reduction=curr_stride,\n",
    "                        module=f'dense_stages.{i}',\n",
    "                        growth_rate=growth_rates[i],\n",
    "                    )\n",
    "                ]\n",
    "            dense_stages.append(nn.Sequential(*dense_stage_layers))\n",
    "        self.dense_stages = nn.Sequential(*dense_stages)\n",
    "\n",
    "        # classifier\n",
    "        self.head = RDNetClassifierHead(num_features, num_classes, drop_rate=drop_rate)\n",
    "\n",
    "        # initialize weights\n",
    "        named_apply(partial(_init_weights, head_init_scale=head_init_scale), self)\n",
    "\n",
    "        if zero_head:\n",
    "            nn.init.zeros_(self.head[-1].weight.data)\n",
    "            if self.head[-1].bias is not None:\n",
    "                nn.init.zeros_(self.head[-1].bias.data)\n",
    "\n",
    "        if checkpoint_path is not None:\n",
    "            self.load_state_dict(torch.load(checkpoint_path, map_location=\"cpu\"))\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def get_classifier(self):\n",
    "        return self.head.fc\n",
    "\n",
    "    def reset_classifier(self, num_classes=0, global_pool=None):\n",
    "        assert global_pool is None\n",
    "        self.head.reset(num_classes)\n",
    "\n",
    "    def forward_head(self, x, pre_logits: bool = False):\n",
    "        return self.head(x, pre_logits=True) if pre_logits else self.head(x)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.dense_stages(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def group_matcher(self, coarse=False):\n",
    "        assert not coarse\n",
    "        return dict(\n",
    "            stem=r'^stem',\n",
    "            blocks=r'^dense_stages\\.(\\d+)',\n",
    "        )\n",
    "\n",
    "\n",
    "def _init_weights(module, name=None, head_init_scale=1.0):\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(module.weight)\n",
    "    elif isinstance(module, nn.BatchNorm2d):\n",
    "        nn.init.constant_(module.weight, 1)\n",
    "        nn.init.constant_(module.bias, 0)\n",
    "    elif isinstance(module, nn.Linear):\n",
    "        nn.init.constant_(module.bias, 0)\n",
    "        if name and 'head.' in name:\n",
    "            module.weight.data.mul_(head_init_scale)\n",
    "            module.bias.data.mul_(head_init_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining TINY_RDNET Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T00:51:47.062705Z",
     "iopub.status.busy": "2025-04-22T00:51:47.061828Z",
     "iopub.status.idle": "2025-04-22T00:51:47.066554Z",
     "shell.execute_reply": "2025-04-22T00:51:47.065933Z",
     "shell.execute_reply.started": "2025-04-22T00:51:47.062679Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "rdnet_tiny_cfg = {\n",
    "    \"url\": \"\",\n",
    "    \"num_classes\": 1000,\n",
    "    \"input_size\": (3, 224, 224),\n",
    "    \"crop_pct\": 0.9,\n",
    "    \"interpolation\": \"bicubic\",\n",
    "    \"mean\": IMAGENET_DEFAULT_MEAN,\n",
    "    \"std\": IMAGENET_DEFAULT_STD,\n",
    "    \"first_conv\": \"stem.0\",\n",
    "    \"classifier\": \"head.fc\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDNet Constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T01:03:37.804730Z",
     "iopub.status.busy": "2025-04-22T01:03:37.804430Z",
     "iopub.status.idle": "2025-04-22T01:03:37.809877Z",
     "shell.execute_reply": "2025-04-22T01:03:37.809310Z",
     "shell.execute_reply.started": "2025-04-22T01:03:37.804710Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def rdnet_tiny(pretrained=False, num_classes=1000, checkpoint_path=None, device=\"cpu\", **kwargs):\n",
    "    n_layer = 7\n",
    "    model_args = {\n",
    "        \"num_init_features\": 64,\n",
    "        \"growth_rates\": [64, 104, 128, 128, 128, 128, 224],\n",
    "        \"num_blocks_list\": [3] * n_layer,\n",
    "        \"is_downsample_block\": (None, True, True, False, False, False, True),\n",
    "        \"transition_compression_ratio\": 0.5,\n",
    "        \"block_type\": [\"Block\", \"Block\", \"BlockESE\", \"BlockESE\", \"BlockESE\", \"BlockESE\", \"BlockESE\"],\n",
    "        \"num_classes\": num_classes,\n",
    "    }\n",
    "\n",
    "    model = RDNet(**{**model_args, **kwargs})\n",
    "\n",
    "    if pretrained:\n",
    "        assert checkpoint_path is not None, \"Please provide checkpoint_path for pretrained weights\"\n",
    "        state_dict = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Loading the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T01:03:41.394533Z",
     "iopub.status.busy": "2025-04-22T01:03:41.393756Z",
     "iopub.status.idle": "2025-04-22T01:03:41.835405Z",
     "shell.execute_reply": "2025-04-22T01:03:41.834799Z",
     "shell.execute_reply.started": "2025-04-22T01:03:41.394509Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = rdnet_tiny(pretrained=True, checkpoint_path=\"rdnet_tiny_pretrained.pth\", device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now resetting the classifier head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T01:03:43.458765Z",
     "iopub.status.busy": "2025-04-22T01:03:43.458208Z",
     "iopub.status.idle": "2025-04-22T01:03:43.499377Z",
     "shell.execute_reply": "2025-04-22T01:03:43.498729Z",
     "shell.execute_reply.started": "2025-04-22T01:03:43.458745Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RDNet(\n",
       "  (stem): PatchifyStem(\n",
       "    (stem): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): LayerNorm2d((64,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (dense_stages): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): DenseStage(\n",
       "        (dense_block0): DenseBlock(\n",
       "          (layers): Block(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64)\n",
       "              (1): LayerNorm2d((64,), eps=1e-06, elementwise_affine=True)\n",
       "              (2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (3): GELU(approximate='none')\n",
       "              (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (dense_block1): DenseBlock(\n",
       "          (layers): Block(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
       "              (1): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
       "              (2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (3): GELU(approximate='none')\n",
       "              (4): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (dense_block2): DenseBlock(\n",
       "          (layers): Block(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "              (1): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "              (2): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (3): GELU(approximate='none')\n",
       "              (4): Conv2d(768, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (2): DenseStage(\n",
       "        (dense_block0): DenseBlock(\n",
       "          (layers): Block(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
       "              (1): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
       "              (2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (3): GELU(approximate='none')\n",
       "              (4): Conv2d(512, 104, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (dense_block1): DenseBlock(\n",
       "          (layers): Block(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(232, 232, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=232)\n",
       "              (1): LayerNorm2d((232,), eps=1e-06, elementwise_affine=True)\n",
       "              (2): Conv2d(232, 928, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (3): GELU(approximate='none')\n",
       "              (4): Conv2d(928, 104, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (dense_block2): DenseBlock(\n",
       "          (layers): Block(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(336, 336, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=336)\n",
       "              (1): LayerNorm2d((336,), eps=1e-06, elementwise_affine=True)\n",
       "              (2): Conv2d(336, 1344, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (3): GELU(approximate='none')\n",
       "              (4): Conv2d(1344, 104, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): LayerNorm2d((440,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(440, 216, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (2): DenseStage(\n",
       "        (dense_block0): DenseBlock(\n",
       "          (layers): BlockESE(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(216, 216, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=216)\n",
       "              (1): LayerNorm2d((216,), eps=1e-06, elementwise_affine=True)\n",
       "              (2): Conv2d(216, 864, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (3): GELU(approximate='none')\n",
       "              (4): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (5): EffectiveSEModule(\n",
       "                (fc): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Hardsigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (dense_block1): DenseBlock(\n",
       "          (layers): BlockESE(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(344, 344, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=344)\n",
       "              (1): LayerNorm2d((344,), eps=1e-06, elementwise_affine=True)\n",
       "              (2): Conv2d(344, 1376, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (3): GELU(approximate='none')\n",
       "              (4): Conv2d(1376, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (5): EffectiveSEModule(\n",
       "                (fc): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Hardsigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (dense_block2): DenseBlock(\n",
       "          (layers): BlockESE(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(472, 472, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=472)\n",
       "              (1): LayerNorm2d((472,), eps=1e-06, elementwise_affine=True)\n",
       "              (2): Conv2d(472, 1888, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (3): GELU(approximate='none')\n",
       "              (4): Conv2d(1888, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (5): EffectiveSEModule(\n",
       "                (fc): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Hardsigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): LayerNorm2d((600,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(600, 296, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): DenseStage(\n",
       "        (dense_block0): DenseBlock(\n",
       "          (layers): BlockESE(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(296, 296, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=296)\n",
       "              (1): LayerNorm2d((296,), eps=1e-06, elementwise_affine=True)\n",
       "              (2): Conv2d(296, 1184, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (3): GELU(approximate='none')\n",
       "              (4): Conv2d(1184, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (5): EffectiveSEModule(\n",
       "                (fc): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Hardsigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (dense_block1): DenseBlock(\n",
       "          (layers): BlockESE(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(424, 424, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=424)\n",
       "              (1): LayerNorm2d((424,), eps=1e-06, elementwise_affine=True)\n",
       "              (2): Conv2d(424, 1696, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (3): GELU(approximate='none')\n",
       "              (4): Conv2d(1696, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (5): EffectiveSEModule(\n",
       "                (fc): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Hardsigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (dense_block2): DenseBlock(\n",
       "          (layers): BlockESE(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(552, 552, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=552)\n",
       "              (1): LayerNorm2d((552,), eps=1e-06, elementwise_affine=True)\n",
       "              (2): Conv2d(552, 2208, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (3): GELU(approximate='none')\n",
       "              (4): Conv2d(2208, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (5): EffectiveSEModule(\n",
       "                (fc): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Hardsigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): LayerNorm2d((680,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(680, 336, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): DenseStage(\n",
       "        (dense_block0): DenseBlock(\n",
       "          (layers): BlockESE(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(336, 336, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=336)\n",
       "              (1): LayerNorm2d((336,), eps=1e-06, elementwise_affine=True)\n",
       "              (2): Conv2d(336, 1344, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (3): GELU(approximate='none')\n",
       "              (4): Conv2d(1344, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (5): EffectiveSEModule(\n",
       "                (fc): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Hardsigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (dense_block1): DenseBlock(\n",
       "          (layers): BlockESE(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(464, 464, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=464)\n",
       "              (1): LayerNorm2d((464,), eps=1e-06, elementwise_affine=True)\n",
       "              (2): Conv2d(464, 1856, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (3): GELU(approximate='none')\n",
       "              (4): Conv2d(1856, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (5): EffectiveSEModule(\n",
       "                (fc): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Hardsigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (dense_block2): DenseBlock(\n",
       "          (layers): BlockESE(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(592, 592, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=592)\n",
       "              (1): LayerNorm2d((592,), eps=1e-06, elementwise_affine=True)\n",
       "              (2): Conv2d(592, 2368, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (3): GELU(approximate='none')\n",
       "              (4): Conv2d(2368, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (5): EffectiveSEModule(\n",
       "                (fc): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Hardsigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): LayerNorm2d((720,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(720, 360, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): DenseStage(\n",
       "        (dense_block0): DenseBlock(\n",
       "          (layers): BlockESE(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(360, 360, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=360)\n",
       "              (1): LayerNorm2d((360,), eps=1e-06, elementwise_affine=True)\n",
       "              (2): Conv2d(360, 1440, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (3): GELU(approximate='none')\n",
       "              (4): Conv2d(1440, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (5): EffectiveSEModule(\n",
       "                (fc): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Hardsigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (dense_block1): DenseBlock(\n",
       "          (layers): BlockESE(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(488, 488, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=488)\n",
       "              (1): LayerNorm2d((488,), eps=1e-06, elementwise_affine=True)\n",
       "              (2): Conv2d(488, 1952, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (3): GELU(approximate='none')\n",
       "              (4): Conv2d(1952, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (5): EffectiveSEModule(\n",
       "                (fc): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Hardsigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (dense_block2): DenseBlock(\n",
       "          (layers): BlockESE(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(616, 616, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=616)\n",
       "              (1): LayerNorm2d((616,), eps=1e-06, elementwise_affine=True)\n",
       "              (2): Conv2d(616, 2464, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (3): GELU(approximate='none')\n",
       "              (4): Conv2d(2464, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (5): EffectiveSEModule(\n",
       "                (fc): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Hardsigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): LayerNorm2d((744,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Conv2d(744, 368, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (2): DenseStage(\n",
       "        (dense_block0): DenseBlock(\n",
       "          (layers): BlockESE(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(368, 368, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=368)\n",
       "              (1): LayerNorm2d((368,), eps=1e-06, elementwise_affine=True)\n",
       "              (2): Conv2d(368, 1472, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (3): GELU(approximate='none')\n",
       "              (4): Conv2d(1472, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (5): EffectiveSEModule(\n",
       "                (fc): Conv2d(224, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Hardsigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (dense_block1): DenseBlock(\n",
       "          (layers): BlockESE(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(592, 592, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=592)\n",
       "              (1): LayerNorm2d((592,), eps=1e-06, elementwise_affine=True)\n",
       "              (2): Conv2d(592, 2368, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (3): GELU(approximate='none')\n",
       "              (4): Conv2d(2368, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (5): EffectiveSEModule(\n",
       "                (fc): Conv2d(224, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Hardsigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (dense_block2): DenseBlock(\n",
       "          (layers): BlockESE(\n",
       "            (layers): Sequential(\n",
       "              (0): Conv2d(816, 816, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=816)\n",
       "              (1): LayerNorm2d((816,), eps=1e-06, elementwise_affine=True)\n",
       "              (2): Conv2d(816, 3264, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (3): GELU(approximate='none')\n",
       "              (4): Conv2d(3264, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (5): EffectiveSEModule(\n",
       "                (fc): Conv2d(224, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Hardsigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): RDNetClassifierHead(\n",
       "    (norm): LayerNorm((1040,), eps=1e-05, elementwise_affine=True)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Linear(in_features=1040, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.reset_classifier(num_classes=10)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a handy evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T01:05:19.812088Z",
     "iopub.status.busy": "2025-04-22T01:05:19.811460Z",
     "iopub.status.idle": "2025-04-22T01:05:19.817211Z",
     "shell.execute_reply": "2025-04-22T01:05:19.816522Z",
     "shell.execute_reply.started": "2025-04-22T01:05:19.812066Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += preds.eq(targets).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T01:22:35.796157Z",
     "iopub.status.busy": "2025-04-22T01:22:35.795574Z",
     "iopub.status.idle": "2025-04-22T01:22:35.812866Z",
     "shell.execute_reply": "2025-04-22T01:22:35.812074Z",
     "shell.execute_reply.started": "2025-04-22T01:22:35.796134Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_rdnet_tiny(model, train_loader, val_loader, test_loader, device, num_epochs=20, \n",
    "                    initial_lr=1e-3, weight_decay=0.05, betas=(0.9, 0.999), \n",
    "                    save_interval=10, resume_path=None, model_name_prefix='rdnet_tiny_transfer_learn',\n",
    "                    early_stopping_patience=10, run_name=\"model\", sub_dir_seed_number='seed_29'):\n",
    "    model.to(device)\n",
    "    warmup_epochs = int(math.ceil((10/100)*num_epochs))\n",
    "\n",
    "    # Optimizer and learning rate scheduler\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=initial_lr, betas=betas, weight_decay=weight_decay)\n",
    "    warmup_scheduler = LinearLR(optimizer, start_factor=0.01, total_iters=warmup_epochs)\n",
    "    cosine_scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=6, T_mult=1, eta_min=1e-6)\n",
    "    scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[warmup_epochs])\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    correspondin_val_acc = 0.0\n",
    "    start_epoch = 0\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    history = {\n",
    "        'epoch': [],\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'test_loss': [],\n",
    "        'test_acc': []\n",
    "    }\n",
    "\n",
    "    # Resume from checkpoint\n",
    "    if resume_path and os.path.exists(resume_path):\n",
    "        checkpoint = torch.load(resume_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        history = checkpoint['history']\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_loss = history['val_loss'][-1]\n",
    "        correspondin_val_acc = history['val_acc'][-1]\n",
    "        print(f\"Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "    try:\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            train_loss = running_loss / total\n",
    "            train_acc = 100. * correct / total\n",
    "\n",
    "            \n",
    "            val_loss, val_acc = evaluate(model, val_loader, device)\n",
    "\n",
    "\n",
    "            test_loss, test_acc = evaluate(model, test_loader, device)\n",
    "\n",
    "            history['epoch'].append(epoch)\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            history['test_loss'].append(test_loss)\n",
    "            history['test_acc'].append(test_acc)\n",
    "\n",
    "            print(f\"Epoch {epoch:03d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | Test Loss: {test_loss:.4f} | \"\n",
    "                  f\"Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "     \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                correspondin_val_acc = val_acc\n",
    "                early_stopping_counter = 0\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'history': history\n",
    "                }\n",
    "                os.makedirs(f'{run_name}/{sub_dir_seed_number}', exist_ok=True)\n",
    "                save_path = os.path.join(f'{run_name}/{sub_dir_seed_number}', f\"{model_name_prefix}__valLoss{val_loss:.4f}_valAcc{correspondin_val_acc:.2f}.pth\")\n",
    "                torch.save(checkpoint, save_path)\n",
    "                print(f\"Saved best checkpoint at {save_path}\")\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                print(f\"No improvement in validation loss for {early_stopping_counter} epoch(s).\")\n",
    "\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Save periodic checkpoints\n",
    "            if epoch % save_interval == 0:\n",
    "                os.makedirs(f'{run_name}/{sub_dir_seed_number}/periodic', exist_ok=True)\n",
    "                save_path = os.path.join(f'{run_name}/{sub_dir_seed_number}/periodic', f\"{model_name_prefix}_epoch{epoch}.pth\")\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'history': history\n",
    "                }, save_path)\n",
    "                print(f\"Saved periodic checkpoint at {save_path}\")\n",
    "\n",
    "            # Early stopping\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "                os.makedirs(f'{run_name}/{sub_dir_seed_number}/earlystopping', exist_ok=True)\n",
    "                save_path = os.path.join(f'{run_name}/{sub_dir_seed_number}/earlystopping', f\"{model_name_prefix}_epoch{epoch}.pth\")\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'history': history\n",
    "                }, save_path)\n",
    "                print(f\"Saved periodic checkpoint at {save_path}\")\n",
    "                break\n",
    "\n",
    "        os.makedirs(f'{run_name}/{sub_dir_seed_number}/last_epoch', exist_ok=True)\n",
    "        save_path = os.path.join(f'{run_name}/{sub_dir_seed_number}/last_epoch', f\"{model_name_prefix}_epoch{epoch}.pth\")\n",
    "        torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'history': history\n",
    "        }, save_path)\n",
    "        return history, correspondin_val_acc\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted! Saving checkpoint...\")\n",
    "        os.makedirs(f'{run_name}/{sub_dir_seed_number}/KeyboardInterrupt', exist_ok=True)\n",
    "        save_path = os.path.join(f'{run_name}/{sub_dir_seed_number}/KeyboardInterrupt', f\"{model_name_prefix}_interrupted_epoch{epoch}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'history': history\n",
    "        }, save_path)\n",
    "        print(f\"Checkpoint saved at {save_path}. You can resume training from here.\")\n",
    "        return history, correspondin_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T01:24:09.850480Z",
     "iopub.status.busy": "2025-04-22T01:24:09.849880Z",
     "iopub.status.idle": "2025-04-22T05:58:43.077188Z",
     "shell.execute_reply": "2025-04-22T05:58:43.076266Z",
     "shell.execute_reply.started": "2025-04-22T01:24:09.850460Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_rdnet_tiny(\n",
    "#     model=model,\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     test_loader=test_loader,\n",
    "#     device=device,\n",
    "#     num_epochs=20,\n",
    "#     initial_lr=1e-3,\n",
    "#     weight_decay=0.05,\n",
    "#     betas=(0.9, 0.999),\n",
    "#     save_interval=10,\n",
    "#     resume_path=None,\n",
    "#     model_name_prefix='rdnet_tiny_transfer_learn',\n",
    "#     early_stopping_patience=10,\n",
    "#     run_name=\"model\",\n",
    "#     sub_dir_seed_number='seed_29'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now since we have the model weights for CIFAR 10 we can directly load them into the RDNet Tiny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T06:19:40.240997Z",
     "iopub.status.busy": "2025-04-22T06:19:40.240125Z",
     "iopub.status.idle": "2025-04-22T06:19:40.676033Z",
     "shell.execute_reply": "2025-04-22T06:19:40.675432Z",
     "shell.execute_reply.started": "2025-04-22T06:19:40.240968Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "path_to_cifar10_weights = \"rdnet_tiny_transfer_learn__valLoss0.1992_valAcc93.86.pth\"\n",
    "model = rdnet_tiny(pretrained=False, num_classes=10)\n",
    "checkpoint = torch.load(path_to_cifar10_weights, map_location=device, weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint configuration is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T06:20:02.835050Z",
     "iopub.status.busy": "2025-04-22T06:20:02.834743Z",
     "iopub.status.idle": "2025-04-22T06:20:02.840017Z",
     "shell.execute_reply": "2025-04-22T06:20:02.839335Z",
     "shell.execute_reply.started": "2025-04-22T06:20:02.835026Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'history'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the model state dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T06:22:26.246119Z",
     "iopub.status.busy": "2025-04-22T06:22:26.245255Z",
     "iopub.status.idle": "2025-04-22T06:22:26.280246Z",
     "shell.execute_reply": "2025-04-22T06:22:26.279724Z",
     "shell.execute_reply.started": "2025-04-22T06:22:26.246093Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "This function evaluates a trained model using a PyTorch DataLoader.\n",
    "\n",
    "It reports:\n",
    "- **F1 Score (Macro)**\n",
    "- **F1 Score (Micro)**\n",
    "- **F1 Score (Weighted)**\n",
    "- **Classification Report**\n",
    "\n",
    "If `show_confusion=True`, it also saves a confusion matrix heatmap in the `charts/` directory.\n",
    "\n",
    "### Example usage:\n",
    "```python\n",
    "evaluate_model(model, val_loader, device, class_names, True, \"val_conf.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T06:25:56.986850Z",
     "iopub.status.busy": "2025-04-22T06:25:56.986148Z",
     "iopub.status.idle": "2025-04-22T06:29:37.948967Z",
     "shell.execute_reply": "2025-04-22T06:29:37.948134Z",
     "shell.execute_reply.started": "2025-04-22T06:25:56.986814Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, class_names=class_names, show_confusion=False, save_path=\"confusion_matrix.png\"):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "    f1_micro = f1_score(all_labels, all_preds, average='micro')\n",
    "    f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(f\"F1 Score (Macro):    {f1_macro:.4f}\")\n",
    "    print(f\"F1 Score (Micro):    {f1_micro:.4f}\")\n",
    "    print(f\"F1 Score (Weighted): {f1_weighted:.4f}\\n\")\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "    if show_confusion:\n",
    "        os.makedirs(\"charts\", exist_ok=True)\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=class_names if class_names else \"auto\",\n",
    "                    yticklabels=class_names if class_names else \"auto\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.ylabel(\"True Label\")\n",
    "        plt.xlabel(\"Predicted Label\")\n",
    "        plt.tight_layout()\n",
    "        full_path = os.path.join(\"charts\", save_path)\n",
    "        plt.savefig(full_path, dpi=600)\n",
    "        plt.close()\n",
    "        print(f\"Confusion matrix saved to: {full_path}\")\n",
    "\n",
    "# print(50*\"=\"+\"\\tStats for val loader\\t\"+\"=\"*50)\n",
    "# evaluate_model(model, val_loader, device, class_names, True, \"val_conf.png\")\n",
    "# print(50*\"=\"+\"\\tStats for test loader\\t\"+\"=\"*50)\n",
    "# evaluate_model(model, test_loader, device, class_names, True, \"test_conf.png\")\n",
    "# print(50*\"=\"+\"\\tStats for train loader\\t\"+\"=\"*50) # very high memory consumptions as images are scaled to imagenet1k level.\n",
    "# evaluate_model(model, train_loader, device, class_names, True, \"train_conf.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "==================================================\tStats for val loader\t==================================================\n",
    "F1 Score (Macro):    0.9386\n",
    "F1 Score (Micro):    0.9386\n",
    "F1 Score (Weighted): 0.9386\n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "    airplane       0.94      0.96      0.95       500\n",
    "  automobile       0.98      0.96      0.97       500\n",
    "        bird       0.91      0.95      0.93       500\n",
    "         cat       0.91      0.87      0.89       500\n",
    "        deer       0.94      0.95      0.94       500\n",
    "         dog       0.90      0.91      0.90       500\n",
    "        frog       0.98      0.93      0.96       500\n",
    "       horse       0.94      0.97      0.95       500\n",
    "        ship       0.92      0.97      0.95       500\n",
    "       truck       0.97      0.93      0.95       500\n",
    "\n",
    "    accuracy                           0.94      5000\n",
    "   macro avg       0.94      0.94      0.94      5000\n",
    "weighted avg       0.94      0.94      0.94      5000\n",
    "\n",
    "Confusion matrix saved to: charts/val_conf.png\n",
    "==================================================\tStats for test loader\t==================================================\n",
    "F1 Score (Macro):    0.9322\n",
    "F1 Score (Micro):    0.9324\n",
    "F1 Score (Weighted): 0.9322\n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "    airplane       0.92      0.96      0.94      1000\n",
    "  automobile       0.96      0.96      0.96      1000\n",
    "        bird       0.90      0.92      0.91      1000\n",
    "         cat       0.90      0.84      0.87      1000\n",
    "        deer       0.93      0.94      0.94      1000\n",
    "         dog       0.91      0.90      0.90      1000\n",
    "        frog       0.97      0.95      0.96      1000\n",
    "       horse       0.94      0.97      0.96      1000\n",
    "        ship       0.94      0.96      0.95      1000\n",
    "       truck       0.96      0.92      0.94      1000\n",
    "\n",
    "    accuracy                           0.93     10000\n",
    "   macro avg       0.93      0.93      0.93     10000\n",
    "weighted avg       0.93      0.93      0.93     10000\n",
    "\n",
    "Confusion matrix saved to: charts/test_conf.png\n",
    "==================================================\tStats for train loader\t==================================================\n",
    "F1 Score (Macro):    0.9852\n",
    "F1 Score (Micro):    0.9852\n",
    "F1 Score (Weighted): 0.9852\n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "    airplane       0.97      0.99      0.98      4500\n",
    "  automobile       0.99      0.99      0.99      4500\n",
    "        bird       0.99      0.99      0.99      4500\n",
    "         cat       0.98      0.97      0.98      4500\n",
    "        deer       0.99      0.99      0.99      4500\n",
    "         dog       0.98      0.98      0.98      4500\n",
    "        frog       1.00      0.99      0.99      4500\n",
    "       horse       0.98      1.00      0.99      4500\n",
    "        ship       0.98      0.99      0.99      4500\n",
    "       truck       0.99      0.97      0.98      4500\n",
    "\n",
    "    accuracy                           0.99     45000\n",
    "   macro avg       0.99      0.99      0.99     45000\n",
    "weighted avg       0.99      0.99      0.99     45000\n",
    "\n",
    "Confusion matrix saved to: charts/train_conf.png\n",
    "</pre>"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
