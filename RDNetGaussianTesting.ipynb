{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python-headless\n",
        "!pip install foolbox"
      ],
      "metadata": {
        "id": "mD7XBgJOHP5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvCN3TCQTyMq",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from torch.utils.data import Dataset, Subset, DataLoader, random_split\n",
        "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingWarmRestarts\n",
        "\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.datasets import CIFAR10, ImageFolder\n",
        "import torchvision.transforms.v2 as v2\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "\n",
        "import re\n",
        "\n",
        "from functools import partial\n",
        "from typing import List\n",
        "\n",
        "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "from timm.layers.squeeze_excite import EffectiveSEModule\n",
        "from timm.models import register_model, build_model_with_cfg, named_apply, generate_default_cfgs\n",
        "from timm.layers import DropPath\n",
        "from timm.layers import LayerNorm2d\n",
        "import timm\n",
        "\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import ssl\n",
        "\n",
        "import foolbox as fb\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nuZkFXXTyMw"
      },
      "outputs": [],
      "source": [
        "if torch.mps.is_available():\n",
        "    device = 'mps'\n",
        "elif torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    device = 'cpu'\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCO_cgseTyMz"
      },
      "outputs": [],
      "source": [
        "seed = 29\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGyNJqr9TyM0"
      },
      "outputs": [],
      "source": [
        "DATA_ROOT = './data.cifar10'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jCnMmY3SB8jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ4j3OLsTyM2"
      },
      "source": [
        "### Model Specific Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dKSXGn2TyM5"
      },
      "outputs": [],
      "source": [
        "timm_model = timm.create_model('rdnet_tiny.nv_in1k', pretrained=True)\n",
        "data_config = timm.data.resolve_model_data_config(timm_model)\n",
        "transform = timm.data.create_transform(**data_config, is_training=True)\n",
        "test_transform = timm.data.create_transform(**data_config, is_training=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57Zi4DfvTyM8"
      },
      "source": [
        "### Getting the pretrained weights. Hugging face models use .bin format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS8cdQZqTyM_"
      },
      "outputs": [],
      "source": [
        "torch.save(timm_model.state_dict(), 'rdnet_tiny_pretrained.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgRjfO1mTyNC"
      },
      "source": [
        "### Reproducing the same transformation locally not relying on timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGtS_pmwTyND"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(\n",
        "        size=(224, 224),\n",
        "        scale=(0.08, 1.0),\n",
        "        ratio=(0.75, 1.3333),\n",
        "        interpolation=InterpolationMode.BICUBIC\n",
        "    ),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(\n",
        "        brightness=(0.6, 1.4),\n",
        "        contrast=(0.6, 1.4),\n",
        "        saturation=(0.6, 1.4)\n",
        "    ),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=(0.485, 0.456, 0.406),\n",
        "        std=(0.229, 0.224, 0.225)\n",
        "    )\n",
        "])\n",
        "\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(\n",
        "        size=248,\n",
        "        interpolation=InterpolationMode.BICUBIC,\n",
        "        antialias=True\n",
        "    ),\n",
        "    transforms.CenterCrop(size=(224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=(0.485, 0.456, 0.406),\n",
        "        std=(0.229, 0.224, 0.225)\n",
        "    )\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EQ8HNAXTyNG"
      },
      "source": [
        "### In case there is a ssl certificate issue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRCdjc1DTyNG"
      },
      "outputs": [],
      "source": [
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLbEHRHCTyNI"
      },
      "source": [
        "### CIFAR 10 DATA DOWNLOAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQr0QVkVTyNJ"
      },
      "outputs": [],
      "source": [
        "trainset = CIFAR10(root=DATA_ROOT, train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = CIFAR10(root=DATA_ROOT, train=False, download=True, transform=test_transform)\n",
        "testloader = DataLoader(testset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPK0Wtz1TyNK"
      },
      "outputs": [],
      "source": [
        "full_dataset = CIFAR10(root=DATA_ROOT, train=True, download=True)\n",
        "targets = np.array(full_dataset.targets)\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=5000, random_state=29)\n",
        "train_idx, val_idx = next(sss.split(np.zeros(len(targets)), targets))\n",
        "\n",
        "train_dataset = Subset(full_dataset, train_idx)\n",
        "val_dataset   = Subset(full_dataset, val_idx)\n",
        "\n",
        "train_dataset.dataset.transform = train_transform\n",
        "val_dataset.dataset.transform   = test_transform\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "test_dataset = CIFAR10(root=DATA_ROOT, train=False, download=True, transform=test_transform)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-ypJi1HTyNM",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title RDNet Implementation\n",
        "__all__ = [\"RDNet\"]\n",
        "\n",
        "\n",
        "class RDNetClassifierHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features: int,\n",
        "        num_classes: int,\n",
        "        drop_rate: float = 0.,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.num_features = in_features\n",
        "\n",
        "        self.norm = nn.LayerNorm(in_features)\n",
        "        self.drop = nn.Dropout(drop_rate)\n",
        "        self.fc = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "    def reset(self, num_classes):\n",
        "        self.fc = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x, pre_logits: bool = False):\n",
        "        x = x.mean([-2, -1])\n",
        "        x = self.norm(x)\n",
        "        x = self.drop(x)\n",
        "        if pre_logits:\n",
        "            return x\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchifyStem(nn.Module):\n",
        "    def __init__(self, num_input_channels, num_init_features, patch_size=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(num_input_channels, num_init_features, kernel_size=patch_size, stride=patch_size),\n",
        "            LayerNorm2d(num_init_features),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.stem(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module): #this is Feature MIXER\n",
        "    \"\"\"D == Dw conv, N == Norm, F == Feed Forward, A == Activation\"\"\"\n",
        "    def __init__(self, in_chs, inter_chs, out_chs):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Conv2d(in_chs, in_chs, groups=in_chs, kernel_size=7, stride=1, padding=3),\n",
        "            LayerNorm2d(in_chs, eps=1e-6),\n",
        "            nn.Conv2d(in_chs, inter_chs, kernel_size=1, stride=1, padding=0),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(inter_chs, out_chs, kernel_size=1, stride=1, padding=0),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class BlockESE(nn.Module): # this is Feature MIXER\n",
        "    \"\"\"D == Dw conv, N == Norm, F == Feed Forward, A == Activation\"\"\"\n",
        "    def __init__(self, in_chs, inter_chs, out_chs):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Conv2d(in_chs, in_chs, groups=in_chs, kernel_size=7, stride=1, padding=3),\n",
        "            LayerNorm2d(in_chs, eps=1e-6),\n",
        "            nn.Conv2d(in_chs, inter_chs, kernel_size=1, stride=1, padding=0),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(inter_chs, out_chs, kernel_size=1, stride=1, padding=0),\n",
        "            EffectiveSEModule(out_chs),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_input_features,\n",
        "        growth_rate,\n",
        "        bottleneck_width_ratio, # I think this is wide part of bottleneck\n",
        "        drop_path_rate,\n",
        "        drop_rate=0.0,\n",
        "        rand_gather_step_prob=0.0,\n",
        "        block_idx=0,\n",
        "        block_type=\"Block\",\n",
        "        ls_init_value=1e-6,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.drop_rate = drop_rate\n",
        "        self.drop_path_rate = drop_path_rate\n",
        "        self.rand_gather_step_prob = rand_gather_step_prob\n",
        "        self.block_idx = block_idx\n",
        "        self.growth_rate = growth_rate\n",
        "\n",
        "        self.gamma = nn.Parameter(ls_init_value * torch.ones(growth_rate)) if ls_init_value > 0 else None\n",
        "        growth_rate = int(growth_rate)\n",
        "        inter_chs = int(num_input_features * bottleneck_width_ratio / 8) * 8\n",
        "\n",
        "        if self.drop_path_rate > 0:\n",
        "            self.drop_path = DropPath(drop_path_rate)\n",
        "\n",
        "        self.layers = eval(block_type)(\n",
        "            in_chs=num_input_features,\n",
        "            inter_chs=inter_chs,\n",
        "            out_chs=growth_rate, # the concatentation of features\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if isinstance(x, List):\n",
        "            x = torch.cat(x, 1)\n",
        "        x = self.layers(x)\n",
        "\n",
        "        if self.gamma is not None:\n",
        "            x = x.mul(self.gamma.reshape(1, -1, 1, 1))\n",
        "\n",
        "        if self.drop_path_rate > 0 and self.training:\n",
        "            x = self.drop_path(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DenseStage(nn.Sequential):\n",
        "    def __init__(self, num_block, num_input_features, drop_path_rates, growth_rate, **kwargs):\n",
        "        super().__init__()\n",
        "        for i in range(num_block):\n",
        "            layer = DenseBlock(\n",
        "                num_input_features=num_input_features,\n",
        "                growth_rate=growth_rate,\n",
        "                drop_path_rate=drop_path_rates[i],\n",
        "                block_idx=i,\n",
        "                **kwargs,\n",
        "            )\n",
        "            num_input_features += growth_rate\n",
        "            self.add_module(f\"dense_block{i}\", layer)\n",
        "        self.num_out_features = num_input_features\n",
        "\n",
        "    def forward(self, init_feature):\n",
        "        features = [init_feature]\n",
        "        for module in self:\n",
        "            new_feature = module(features)\n",
        "            features.append(new_feature)\n",
        "        return torch.cat(features, 1)\n",
        "\n",
        "\n",
        "class RDNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_init_features=64,\n",
        "        growth_rates=(64, 104, 128, 128, 128, 128, 224),\n",
        "        num_blocks_list=(3, 3, 3, 3, 3, 3, 3),\n",
        "        bottleneck_width_ratio=4,\n",
        "        zero_head=False,\n",
        "        in_chans=3,  # timm option [--in-chans]\n",
        "        num_classes=1000,  # timm option [--num-classes]\n",
        "        drop_rate=0.0,  # timm option [--drop: dropout ratio]\n",
        "        drop_path_rate=0.0,  # timm option [--drop-path: drop-path ratio]\n",
        "        checkpoint_path=None,  # timm option [--initial-checkpoint]\n",
        "        transition_compression_ratio=0.5,\n",
        "        ls_init_value=1e-6,\n",
        "        is_downsample_block=(None, True, True, False, False, False, True),\n",
        "        block_type=\"Block\",\n",
        "        head_init_scale: float = 1.,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert len(growth_rates) == len(num_blocks_list) == len(is_downsample_block)\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        if isinstance(block_type, str):\n",
        "            block_type = [block_type] * len(growth_rates)\n",
        "\n",
        "        # stem\n",
        "        self.stem = PatchifyStem(in_chans, num_init_features, patch_size=4)\n",
        "\n",
        "        # features\n",
        "        self.feature_info = []\n",
        "        self.num_stages = len(growth_rates)\n",
        "        curr_stride = 4  # stem_stride\n",
        "        num_features = num_init_features\n",
        "        dp_rates = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(num_blocks_list)).split(num_blocks_list)]\n",
        "\n",
        "        dense_stages = []\n",
        "        for i in range(self.num_stages):\n",
        "            dense_stage_layers = []\n",
        "            if i != 0:\n",
        "                compressed_num_features = int(num_features * transition_compression_ratio / 8) * 8\n",
        "                k_size = stride = 1\n",
        "                if is_downsample_block[i]:\n",
        "                    curr_stride *= 2\n",
        "                    k_size = stride = 2\n",
        "                dense_stage_layers.append(LayerNorm2d(num_features))\n",
        "                dense_stage_layers.append(\n",
        "                    nn.Conv2d(num_features, compressed_num_features, kernel_size=k_size, stride=stride, padding=0)\n",
        "                )\n",
        "                num_features = compressed_num_features\n",
        "\n",
        "            stage = DenseStage(\n",
        "                num_block=num_blocks_list[i],\n",
        "                num_input_features=num_features,\n",
        "                growth_rate=growth_rates[i],\n",
        "                bottleneck_width_ratio=bottleneck_width_ratio,\n",
        "                drop_rate=drop_rate,\n",
        "                drop_path_rates=dp_rates[i],\n",
        "                ls_init_value=ls_init_value,\n",
        "                block_type=block_type[i],\n",
        "            )\n",
        "            dense_stage_layers.append(stage)\n",
        "            num_features += num_blocks_list[i] * growth_rates[i]\n",
        "\n",
        "            if i + 1 == self.num_stages or (i + 1 != self.num_stages and is_downsample_block[i + 1]):\n",
        "                self.feature_info += [\n",
        "                    dict(\n",
        "                        num_chs=num_features,\n",
        "                        reduction=curr_stride,\n",
        "                        module=f'dense_stages.{i}',\n",
        "                        growth_rate=growth_rates[i],\n",
        "                    )\n",
        "                ]\n",
        "            dense_stages.append(nn.Sequential(*dense_stage_layers))\n",
        "        self.dense_stages = nn.Sequential(*dense_stages)\n",
        "\n",
        "        # classifier\n",
        "        self.head = RDNetClassifierHead(num_features, num_classes, drop_rate=drop_rate)\n",
        "\n",
        "        # initialize weights\n",
        "        named_apply(partial(_init_weights, head_init_scale=head_init_scale), self)\n",
        "\n",
        "        if zero_head:\n",
        "            nn.init.zeros_(self.head[-1].weight.data)\n",
        "            if self.head[-1].bias is not None:\n",
        "                nn.init.zeros_(self.head[-1].bias.data)\n",
        "\n",
        "        if checkpoint_path is not None:\n",
        "            self.load_state_dict(torch.load(checkpoint_path, map_location=\"cpu\"))\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def get_classifier(self):\n",
        "        return self.head.fc\n",
        "\n",
        "    def reset_classifier(self, num_classes=0, global_pool=None):\n",
        "        assert global_pool is None\n",
        "        self.head.reset(num_classes)\n",
        "\n",
        "    def forward_head(self, x, pre_logits: bool = False):\n",
        "        return self.head(x, pre_logits=True) if pre_logits else self.head(x)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.dense_stages(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "    def group_matcher(self, coarse=False):\n",
        "        assert not coarse\n",
        "        return dict(\n",
        "            stem=r'^stem',\n",
        "            blocks=r'^dense_stages\\.(\\d+)',\n",
        "        )\n",
        "\n",
        "\n",
        "def _init_weights(module, name=None, head_init_scale=1.0):\n",
        "    if isinstance(module, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(module.weight)\n",
        "    elif isinstance(module, nn.BatchNorm2d):\n",
        "        nn.init.constant_(module.weight, 1)\n",
        "        nn.init.constant_(module.bias, 0)\n",
        "    elif isinstance(module, nn.Linear):\n",
        "        nn.init.constant_(module.bias, 0)\n",
        "        if name and 'head.' in name:\n",
        "            module.weight.data.mul_(head_init_scale)\n",
        "            module.bias.data.mul_(head_init_scale)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqvYvMi_TyNQ"
      },
      "source": [
        "### Defining TINY_RDNET Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zz_CuDACTyNR"
      },
      "outputs": [],
      "source": [
        "IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
        "\n",
        "rdnet_tiny_cfg = {\n",
        "    \"url\": \"\",  # optional: local path to weights if needed\n",
        "    \"num_classes\": 1000,\n",
        "    \"input_size\": (3, 224, 224),\n",
        "    \"crop_pct\": 0.9,\n",
        "    \"interpolation\": \"bicubic\",\n",
        "    \"mean\": IMAGENET_DEFAULT_MEAN,\n",
        "    \"std\": IMAGENET_DEFAULT_STD,\n",
        "    \"first_conv\": \"stem.0\",\n",
        "    \"classifier\": \"head.fc\",\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UMEYThWTyNS"
      },
      "source": [
        "### RDNet Constructor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzqEa7H3TyNS"
      },
      "outputs": [],
      "source": [
        "def rdnet_tiny(pretrained=False, num_classes=1000, checkpoint_path=None, device=\"cpu\", **kwargs):\n",
        "    n_layer = 7\n",
        "    model_args = {\n",
        "        \"num_init_features\": 64,\n",
        "        \"growth_rates\": [64, 104, 128, 128, 128, 128, 224],\n",
        "        \"num_blocks_list\": [3] * n_layer,\n",
        "        \"is_downsample_block\": (None, True, True, False, False, False, True),\n",
        "        \"transition_compression_ratio\": 0.5,\n",
        "        \"block_type\": [\"Block\", \"Block\", \"BlockESE\", \"BlockESE\", \"BlockESE\", \"BlockESE\", \"BlockESE\"],\n",
        "        \"num_classes\": num_classes,\n",
        "    }\n",
        "\n",
        "    model = RDNet(**{**model_args, **kwargs})\n",
        "\n",
        "    if pretrained:\n",
        "        assert checkpoint_path is not None, \"Please provide checkpoint_path for pretrained weights\"\n",
        "        state_dict = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "        model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWG4IJrQTyNU"
      },
      "source": [
        "### First Loading the model weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyTkYK97TyNV"
      },
      "outputs": [],
      "source": [
        "model = rdnet_tiny(pretrained=True, checkpoint_path=\"rdnet_tiny_pretrained.pth\", device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvL8H6mZCmW4"
      },
      "outputs": [],
      "source": [
        "full_dataset = CIFAR10(root=DATA_ROOT, train=True, download=True)\n",
        "targets = np.array(full_dataset.targets)\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=5000, random_state=29)\n",
        "train_idx, val_idx = next(sss.split(np.zeros(len(targets)), targets))\n",
        "\n",
        "train_dataset = Subset(full_dataset, train_idx)\n",
        "val_dataset   = Subset(full_dataset, val_idx)\n",
        "\n",
        "train_dataset.dataset.transform = train_transform\n",
        "val_dataset.dataset.transform   = test_transform\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "test_dataset = CIFAR10(root=DATA_ROOT, train=False, download=True, transform=test_transform)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGHnm34kCYS0"
      },
      "outputs": [],
      "source": [
        "# To load the model\n",
        "path_to_cifar10_weights = \"rdnet_tiny_transfer_learn__valLoss0.1992_valAcc93.86.pth\"\n",
        "model = rdnet_tiny(pretrained=False, num_classes=10)\n",
        "checkpoint = torch.load(path_to_cifar10_weights, map_location=device, weights_only=False)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdFZNxpQCrHl"
      },
      "outputs": [],
      "source": [
        "def generate_adversarial_dataset(model, data_loader, device, attack, epsilon=0.03, num_batches=None, save_as_pt=None):\n",
        "    \"\"\"\n",
        "    Generate adversarial examples using Foolbox and return them as a list.\n",
        "    \"\"\"\n",
        "    model.eval().to(device)\n",
        "    fmodel = fb.PyTorchModel(model, bounds=(0, 1))\n",
        "    adv_examples = []\n",
        "\n",
        "    for i, (images, labels) in enumerate(data_loader):\n",
        "        if num_batches is not None and i >= num_batches:\n",
        "            break\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        _, clipped_advs, _ = attack(fmodel, images, labels, epsilons=epsilon)\n",
        "\n",
        "        for adv_img, label in zip(clipped_advs, labels):\n",
        "            adv_examples.append((adv_img.detach().cpu(), label.cpu()))\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    if save_as_pt:\n",
        "        torch.save(adv_examples, save_as_pt)\n",
        "        print(f\"Adversarial dataset saved to '{save_as_pt}'\")\n",
        "\n",
        "    return adv_examples\n",
        "\n",
        "class AdversarialTensorDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Wraps a list of adversarial image-label pairs into a PyTorch Dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "#attack = fb.attacks.FGSM()\n",
        "#attack = fb.attacks.L2DeepFoolAttack()\n",
        "#attack = fb.attacks.LinfPGD()\n",
        "attack = fb.attacks.L2PGD()\n",
        "\n",
        "val_transform_no_norm = transforms.Compose([\n",
        "    transforms.Resize(size=248, interpolation=InterpolationMode.BICUBIC, antialias=True),\n",
        "    transforms.CenterCrop(size=(224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "val_dataset_no_norm_base = CIFAR10(root=DATA_ROOT, train=True, download=True, transform=val_transform_no_norm)\n",
        "val_dataset_no_norm = Subset(val_dataset_no_norm_base, val_idx)\n",
        "\n",
        "val_loader_no_norm = DataLoader(val_dataset_no_norm, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "adv_data = generate_adversarial_dataset(\n",
        "    model,\n",
        "    val_loader_no_norm,\n",
        "    device,\n",
        "    attack,\n",
        "    epsilon=0.03,\n",
        "    num_batches=10,\n",
        "    save_as_pt=\"fgsm_adv_dataset.pt\"\n",
        ")\n",
        "\n",
        "adv_dataset = AdversarialTensorDataset(adv_data)\n",
        "adv_loader = DataLoader(adv_dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CreAOHvFute"
      },
      "outputs": [],
      "source": [
        "GAUSSIAN_KERNEL_SIZE = (5, 5) # Must be odd integers\n",
        "GAUSSIAN_SIGMA = 0          # If 0, sigma is calculated from kernel size\n",
        "\n",
        "def evaluate_accuracy(model, data_loader_or_batch, device):\n",
        "    \"\"\"Calculates model accuracy on a DataLoader or a single batch.\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if isinstance(data_loader_or_batch, DataLoader):\n",
        "             data_iter = data_loader_or_batch\n",
        "        elif isinstance(data_loader_or_batch, (tuple, list)) and len(data_loader_or_batch) == 2:\n",
        "             data_iter = [data_loader_or_batch]\n",
        "        else:\n",
        "            raise ValueError(\"Input must be a DataLoader or a tuple/list of (images, labels)\")\n",
        "\n",
        "        for images, labels in data_iter:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    if total == 0:\n",
        "        return 0.0\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# 1. Get one batch of adversarial data\n",
        "try:\n",
        "    images_adv, labels_adv = next(iter(adv_loader))\n",
        "    print(f\"Fetched one batch of {len(images_adv)} adversarial examples.\")\n",
        "except StopIteration:\n",
        "    print(\"Adversarial DataLoader is empty. Cannot proceed.\")\n",
        "    exit()\n",
        "except NameError:\n",
        "    print(\"Error: 'adv_loader' not defined. Make sure you ran the previous code cells.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred while fetching data: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 2. Split the batch into Group A and Group B\n",
        "batch_size = images_adv.size(0)\n",
        "if batch_size < 2:\n",
        "    print(\"Batch size is too small (< 2) to split. Stopping.\")\n",
        "    exit()\n",
        "\n",
        "split_idx = batch_size // 2\n",
        "\n",
        "images_a = images_adv[:split_idx]\n",
        "labels_a = labels_adv[:split_idx]\n",
        "\n",
        "images_b = images_adv[split_idx:]\n",
        "labels_b = labels_adv[split_idx:]\n",
        "\n",
        "print(f\"Split batch: Group A ({len(images_a)} samples), Group B ({len(images_b)} samples)\")\n",
        "\n",
        "# 3. Evaluate accuracy on Group A (original adversarial)\n",
        "print(\"\\nEvaluating accuracy on Group A (Adversarial)...\")\n",
        "try:\n",
        "    accuracy_a = evaluate_accuracy(model, (images_a, labels_a), device)\n",
        "    print(f\"Accuracy on Group A: {accuracy_a:.2f}%\")\n",
        "except NameError:\n",
        "     print(\"Error: 'model' or 'device' not defined.\")\n",
        "     exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during Group A evaluation: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 4. Apply Gaussian Blur to Group B images\n",
        "print(f\"\\nApplying Gaussian Blur (kernel={GAUSSIAN_KERNEL_SIZE}, sigma={GAUSSIAN_SIGMA}) to Group B images...\")\n",
        "images_b_blurred_list = []\n",
        "for img_tensor in images_b:\n",
        "    # Convert PyTorch tensor to NumPy array and ensure it's float32\n",
        "    img_np = img_tensor.permute(1, 2, 0).cpu().numpy().astype(np.float32)\n",
        "\n",
        "    # Apply Gaussian Blur using OpenCV\n",
        "    img_blurred_np = cv2.GaussianBlur(img_np, GAUSSIAN_KERNEL_SIZE, GAUSSIAN_SIGMA)\n",
        "\n",
        "    # Handle potential clipping if blur pushes values outside [0, 1] slightly\n",
        "    img_blurred_np = np.clip(img_blurred_np, 0, 1)\n",
        "\n",
        "    # Add channel dimension back if it was squeezed by cv2 (for grayscale)\n",
        "    if img_blurred_np.ndim == 2:\n",
        "        img_blurred_np = np.expand_dims(img_blurred_np, axis=-1)\n",
        "\n",
        "    # Convert back to PyTorch tensor (C, H, W)\n",
        "    img_blurred_tensor = torch.from_numpy(img_blurred_np).permute(2, 0, 1)\n",
        "    images_b_blurred_list.append(img_blurred_tensor)\n",
        "\n",
        "# Stack the list of blurred tensors back into a single batch tensor\n",
        "if images_b_blurred_list:\n",
        "    images_b_blurred = torch.stack(images_b_blurred_list)\n",
        "    print(\"Gaussian blur applied successfully.\")\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(images_b[0].permute(1, 2, 0).cpu().numpy())\n",
        "    plt.title(\"Original Adversarial (Group B)\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(images_b_blurred[0].permute(1, 2, 0).cpu().numpy())\n",
        "    plt.title(\"Blurred Adversarial (Group B)\")\n",
        "    plt.axis('off')\n",
        "    plt.suptitle(\"Example Before/After Gaussian Blur\")\n",
        "    plt.show()\n",
        "\n",
        "    # 5. Evaluate accuracy on Group B (blurred adversarial)\n",
        "    print(\"\\nEvaluating accuracy on Group B (Blurred Adversarial)...\")\n",
        "    try:\n",
        "        accuracy_b = evaluate_accuracy(model, (images_b_blurred, labels_b), device)\n",
        "        print(f\"Accuracy on Group B (Blurred): {accuracy_b:.2f}%\")\n",
        "\n",
        "        # 6. Compare Accuracies\n",
        "        print(\"\\n--- Comparison ---\")\n",
        "        print(f\"Accuracy on Adversarial (Group A):      {accuracy_a:.2f}%\")\n",
        "        print(f\"Accuracy on Blurred Adversarial (Group B): {accuracy_b:.2f}%\")\n",
        "        if accuracy_b > accuracy_a:\n",
        "             print(\"Gaussian blur improved accuracy against these adversarial examples.\")\n",
        "        elif accuracy_b < accuracy_a:\n",
        "             print(\"Gaussian blur decreased accuracy against these adversarial examples.\")\n",
        "        else:\n",
        "             print(\"Gaussian blur had no significant effect on accuracy for these adversarial examples.\")\n",
        "\n",
        "    except NameError:\n",
        "        print(\"Error: 'model' or 'device' not defined.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during Group B evaluation: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"Group B was empty after splitting, cannot apply blur or evaluate.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTG3oo6ROdV8"
      },
      "outputs": [],
      "source": [
        "NUM_EVAL_RUNS = 10\n",
        "GAUSSIAN_KERNEL_SIZE = (5, 5) # Must be odd integers\n",
        "GAUSSIAN_SIGMA = 0          # If 0, sigma is calculated from kernel size\n",
        "\n",
        "print(f\"\\n--- Running {NUM_EVAL_RUNS} Evaluation Runs on NEW Random Batches ---\")\n",
        "print(f\"Applying Gaussian Blur (kernel={GAUSSIAN_KERNEL_SIZE}, sigma={GAUSSIAN_SIGMA}) to Group B of each batch.\")\n",
        "\n",
        "try:\n",
        "    _ = model\n",
        "    _ = device\n",
        "    _ = adv_loader\n",
        "    _ = evaluate_accuracy\n",
        "    if not isinstance(adv_loader, DataLoader):\n",
        "         raise NameError(\"'adv_loader' is not a PyTorch DataLoader\")\n",
        "    print(\"Prerequisites (model, device, adv_loader, evaluate_accuracy) found.\")\n",
        "    print(\"Assuming 'adv_loader' was created with shuffle=True for random batch selection.\")\n",
        "\n",
        "except NameError as e:\n",
        "    print(f\"Error: A required variable or function is not defined: {e}\")\n",
        "    print(\"Please ensure the previous code cells defining these have been run successfully.\")\n",
        "    raise e\n",
        "\n",
        "\n",
        "# Perform Multiple Evaluation Runs on Different Batches\n",
        "accuracies_a_runs = [] # Store Group A accuracy for each batch\n",
        "accuracies_b_runs = [] # Store Group B accuracy for each batch\n",
        "run_times = []\n",
        "batches_processed = 0\n",
        "\n",
        "# Create a single iterator for the loader before the loop\n",
        "try:\n",
        "    adv_iterator = iter(adv_loader)\n",
        "\n",
        "    for i in range(NUM_EVAL_RUNS):\n",
        "        print(f\"\\n--- Processing Run {i+1}/{NUM_EVAL_RUNS} ---\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        # 1. Get a new batch of adversarial data\n",
        "        try:\n",
        "            images_adv, labels_adv = next(adv_iterator)\n",
        "            print(f\"Fetched batch {i+1} with {len(images_adv)} samples.\")\n",
        "            batches_processed += 1\n",
        "        except StopIteration:\n",
        "            print(f\"\\nAdversarial DataLoader exhausted after {batches_processed} batches. Stopping early.\")\n",
        "            NUM_EVAL_RUNS = batches_processed\n",
        "            break\n",
        "\n",
        "        # 2. Split the current batch into Group A and Group B\n",
        "        batch_size = images_adv.size(0)\n",
        "        if batch_size < 2:\n",
        "            print(f\"Batch size ({batch_size}) is too small to split. Skipping run {i+1}.\")\n",
        "            continue\n",
        "\n",
        "        split_idx = batch_size // 2\n",
        "        images_a = images_adv[:split_idx]\n",
        "        labels_a = labels_adv[:split_idx]\n",
        "        images_b = images_adv[split_idx:]\n",
        "        labels_b = labels_adv[split_idx:]\n",
        "\n",
        "        # 3. Evaluate accuracy on Group A (original adversarial) for this batch\n",
        "        try:\n",
        "            current_accuracy_a = evaluate_accuracy(model, (images_a, labels_a), device)\n",
        "            accuracies_a_runs.append(current_accuracy_a)\n",
        "            print(f\"  Accuracy on Group A (Adversarial): {current_accuracy_a:.2f}%\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during Group A evaluation for run {i+1}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # 4. Apply Gaussian Blur to Group B images for this batch\n",
        "        images_b_blurred_list = []\n",
        "        try:\n",
        "            for img_tensor in images_b:\n",
        "                img_np = img_tensor.permute(1, 2, 0).cpu().numpy().astype(np.float32)\n",
        "                img_blurred_np = cv2.GaussianBlur(img_np, GAUSSIAN_KERNEL_SIZE, GAUSSIAN_SIGMA)\n",
        "                img_blurred_np = np.clip(img_blurred_np, 0, 1)\n",
        "                if img_blurred_np.ndim == 2: img_blurred_np = np.expand_dims(img_blurred_np, axis=-1)\n",
        "                img_blurred_tensor = torch.from_numpy(img_blurred_np).permute(2, 0, 1)\n",
        "                images_b_blurred_list.append(img_blurred_tensor)\n",
        "\n",
        "            if not images_b_blurred_list:\n",
        "                 print(\"  Group B is empty after split. Skipping blur/eval for Group B.\")\n",
        "                 accuracies_b_runs.append(np.nan)\n",
        "                 continue\n",
        "\n",
        "            images_b_blurred = torch.stack(images_b_blurred_list)\n",
        "\n",
        "            # 5. Evaluate accuracy on Group B (blurred adversarial) for this batch\n",
        "            current_accuracy_b = evaluate_accuracy(model, (images_b_blurred, labels_b), device)\n",
        "            accuracies_b_runs.append(current_accuracy_b)\n",
        "            print(f\"  Accuracy on Group B (Blurred): {current_accuracy_b:.2f}%\")\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"An error occurred during Group B processing/evaluation for run {i+1}: {e}\")\n",
        "             accuracies_b_runs.append(np.nan)\n",
        "             continue\n",
        "\n",
        "        # Memory cleanup\n",
        "        del images_adv, labels_adv, images_a, labels_a, images_b, labels_b\n",
        "        if 'images_b_blurred' in locals(): del images_b_blurred\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        end_time = time.time()\n",
        "        run_times.append(end_time - start_time)\n",
        "        print(f\"  Run {i+1} completed in {run_times[-1]:.4f}s\")\n",
        "\n",
        "\n",
        "except NameError as e:\n",
        "     print(f\"Error initializing: Required variable '{e.name}' not defined.\")\n",
        "     raise e\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "    raise e\n",
        "\n",
        "\n",
        "# Calculate Statistics\n",
        "print(\"\\n--- Overall Statistics Across Processed Batches ---\")\n",
        "\n",
        "if batches_processed > 0 and accuracies_b_runs:\n",
        "    valid_accuracies_a = [acc for acc in accuracies_a_runs if not np.isnan(acc)]\n",
        "    valid_accuracies_b = [acc for acc in accuracies_b_runs if not np.isnan(acc)]\n",
        "    num_valid_runs = len(valid_accuracies_b)\n",
        "\n",
        "    if num_valid_runs > 0:\n",
        "        accuracies_a_np = np.array(valid_accuracies_a)\n",
        "        accuracies_b_np = np.array(valid_accuracies_b)\n",
        "\n",
        "        mean_accuracy_a = np.mean(accuracies_a_np)\n",
        "        std_dev_accuracy_a = np.std(accuracies_a_np)\n",
        "        mean_accuracy_b = np.mean(accuracies_b_np)\n",
        "        std_dev_accuracy_b = np.std(accuracies_b_np)\n",
        "\n",
        "        print(f\"Number of Batches Successfully Processed: {num_valid_runs} (out of {NUM_EVAL_RUNS} requested)\")\n",
        "\n",
        "        print(\"\\nGroup A (Adversarial):\")\n",
        "        print(f\"  Average Accuracy:    {mean_accuracy_a:.2f}%\")\n",
        "        print(f\"  Standard Deviation:  {std_dev_accuracy_a:.4f}\")\n",
        "        print(f\"  Min/Max Accuracy:    {np.min(accuracies_a_np):.2f}% / {np.max(accuracies_a_np):.2f}%\")\n",
        "\n",
        "        print(\"\\nGroup B (Blurred Adversarial):\")\n",
        "        print(f\"  Average Accuracy:    {mean_accuracy_b:.2f}%\")\n",
        "        print(f\"  Standard Deviation:  {std_dev_accuracy_b:.4f}\")\n",
        "        print(f\"  Min/Max Accuracy:    {np.min(accuracies_b_np):.2f}% / {np.max(accuracies_b_np):.2f}%\")\n",
        "\n",
        "        print(f\"\\nAverage Processing Time per Batch: {np.mean(run_times):.4f}s\")\n",
        "\n",
        "        plt.figure(figsize=(12, 7))\n",
        "\n",
        "        run_indices = range(1, num_valid_runs + 1)\n",
        "\n",
        "        # Plot Group A accuracies\n",
        "        plt.plot(run_indices, accuracies_a_np, marker='x', linestyle=':', color='red', label=f'Group A Accuracy (Adv. Avg: {mean_accuracy_a:.2f}%)')\n",
        "        plt.axhline(mean_accuracy_a, color='darkred', linestyle=':', alpha=0.8, label='Mean Group A Accuracy')\n",
        "\n",
        "        # Plot Group B accuracies\n",
        "        plt.plot(run_indices, accuracies_b_np, marker='o', linestyle='-', color='blue', label=f'Group B Accuracy (Blurred Avg: {mean_accuracy_b:.2f}%)')\n",
        "        plt.axhline(mean_accuracy_b, color='darkblue', linestyle='--', label='Mean Group B Accuracy')\n",
        "\n",
        "        plt.title(f'Accuracy per Batch Across {num_valid_runs} Runs')\n",
        "        plt.xlabel('Batch Run Number')\n",
        "        plt.ylabel('Accuracy (%)')\n",
        "        # Calculate reasonable y-limits based on data\n",
        "        all_accs = np.concatenate((accuracies_a_np, accuracies_b_np))\n",
        "        min_plot_acc = max(0, np.min(all_accs) - 5)\n",
        "        max_plot_acc = min(100, np.max(all_accs) + 5)\n",
        "        plt.ylim(min_plot_acc, max_plot_acc)\n",
        "\n",
        "        if num_valid_runs <= 20:\n",
        "             plt.xticks(run_indices)\n",
        "        else:\n",
        "             tick_indices = np.linspace(1, num_valid_runs, num=min(num_valid_runs, 10), dtype=int)\n",
        "             plt.xticks(tick_indices)\n",
        "\n",
        "\n",
        "        plt.legend()\n",
        "        plt.grid(True, linestyle='--', alpha=0.6)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    else:\n",
        "        print(\"No batches were successfully processed completely (check for errors during runs).\")\n",
        "\n",
        "else:\n",
        "    print(\"No batches were processed. Cannot calculate statistics or plot.\")\n",
        "    if not adv_loader: print(\"Check if 'adv_loader' was properly initialized.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31011,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}